{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree from Scratch**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, you will learn everything you need to know about Decision Trees and how they work under the hood. Hope you'll find this helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Table Of Contents**\n",
    "\n",
    "1. [Introduction to Decision Trees](#1)\n",
    "2. [How does the Decision Tree Algorithm Work](#2)\n",
    "3. [What is Information Gain and How do we measure it in Decision Tree](#3)\n",
    "4. [Evaluating the Complexity of Decision Tree Algorithm](#4)\n",
    "5. [Pros and Cons of Decision Trees](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to Decision Trees** <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are **Decision Trees**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees** are non-parametric (model complexity can grow with the amount of data allowing it to capture more complex patterns) supervised learning algorithm, \n",
    "which can be use for both Classification and Regression tasks. It has a hierachical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Decision Trees component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Root Node** : Very first node of the tree it represents the entire dataset and the initial decision made by the algorithm.\n",
    "\n",
    "- **Branches** : Represent the result of a node decision. For example at a node level we can have x1 > 5 the result of this can be either True or False, so we will have two branches, one for the case the result is True and the other for the case the result is False.\n",
    "\n",
    "- **Internal Nodes** : Represent decision points where the data is split based on feature's value. These nodes apply conditions to the features in the dataset and direct the data down to leaf nodes.\n",
    "\n",
    "- **Leaf Nodes** : Represent the final prediction, no further splits happen at this nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DecisionTree.png\" style=\"display: block; margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot convert every set of rules to decision trees, but you can convert every decision trees to a set of rules. That is why decision trees are good models for **Machine Learning Explainability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. How does the Decision Tree Algorithm Work** <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Algorithm follow a step by step process that helps classify data points or predict value by splitting the dataset recursively into smaller more manageable parts. Here is an overview of how it works: \n",
    "\n",
    "\n",
    "- **Starting with the whole Dataset** : The algorithm begins by considering the entire dataset as one big group. It looks for the best way to split this data into smaller groups based on one of the features(variables) in the dataset.\n",
    "\n",
    "\n",
    "Now a question you may ask is how does it choose which of the features is the best for splitting the dataset??\n",
    "\n",
    "\n",
    "- **Choosing the best feature to split**: The algorithm examines all the features and chooses the one that splits the data the best. It uses a measure called **Information Gain** to decide this. I will talk about Information Gain in the next section, for now just understand it like this. Information Gain helps the algorithm find the feature that creates the most **\"purity\"** in the data after the split. **Purity** means that after splitting, most of the data points in each group belong to a single class (for classification tasks) or have similar values (for regression tasks).\n",
    "\n",
    "\n",
    "- **Recursively Splitting the Data**: Once the first split is made, the algorithm repeats the process for each of the resulting groups. It keeps splitting the data recursively, creating smaller and smaller groups, and building branches of the tree along the way. At each step, it looks for the feature that gives the highest Information Gain.\n",
    "\n",
    "\n",
    "At this point you understand the general approach of Decision Trees and you understand it is a recursive algorithm, but we missed something, every recursive algorithm need a stopping criteria, and Decision Trees are no exceptions.\n",
    "\n",
    "\n",
    "- **Stopping Conditions**: The algorithm continues splitting until it reaches one of the stopping points: \n",
    "\n",
    "    - **Pure Nodes**: If a node is \"pure\"(all data points belong to the same class or have very similar values), there's no need to split it further.\n",
    "\n",
    "    - **Information Gain is too small**: If the information Gain from a potential split is too low(indicating the split won't improve the tree), the algorithm stops splitting that branch.\n",
    "\n",
    "    - **Maximum depth or other constraints**: A maximum depth can be specified for the tree to prevent overfitting but more on that later.\n",
    "    \n",
    "\n",
    "\n",
    "- **Making Predictions** : Now that our Decision Tree is fully grown, it can used to make predictions. To classify a new data point, the decision tree starts at the root node and follows the splits based on the data point's feature values until it reaches a leaf node. The class or value at that leaf node is the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. What is Information Gain and How do we measure it in Decision Tree**  <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we explain that the Decision Tree algorithm recursively split the data based on the best features and that it uses something call **Information Gain** to determine which feature is the best. But we didn't explain what is Information Gain and how do we measure it in Decision Tree. So let's do it now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Gain** : is a metric used in decision trees to determine how well a feature separates the data into distinct classes. It measures the reduction in uncertainty or entropy when the dataset is split based on a specific feature. The higher the information Gain, the more effective the feature is at classifying the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we understand what Information Gain is, but we still don't know how to measure it in Decision Tree. If we take a closer look at the definition we can read **\"It measeures the reduction in uncertainty or entropy\"**. So entropy can be used to calculate Information Gain, here is the formula: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\text{GAIN}(\\mathcal{D}, x_j) = H(\\mathcal{D}) - \\sum_{v \\in \\text{Values}(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"InformationGain.png\" style=\"display: block; margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with the formula to make sure we understand everything: \n",
    "\n",
    "Example: Let's compute the Entropy and Gain Information for the following Decision Tree.\n",
    "\n",
    "<img src=\"Entropy.png\" style=\"display: block; margin-left: auto; margin-right: auto;\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Entropy's formula is : $H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the root node we have: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "y = 1: 60, \\quad y = 0: 40 \\\\\n",
    "\\text{Total examples} = 60 + 40 = 100 \\\\[10pt]\n",
    "\n",
    "p_1 = \\frac{60}{100} = 0.6 \\\\[10pt]\n",
    "p_0 = \\frac{40}{100} = 0.4 \\\\[10pt]\n",
    "\n",
    "\\log_2(p_1) = \\log_2(0.6) \\approx -0.737 \\\\[10pt]\n",
    "\\log_2(p_0) = \\log_2(0.4) \\approx -1.322 \\\\[10pt]\n",
    "\n",
    "H(S) = -(p_1 \\log_2(p_1) + p_0 \\log_2(p_0)) \\\\[10pt]\n",
    "\n",
    "H(S) = -(0.6 \\times (-0.737) + 0.4 \\times (-1.322)) \\\\[10pt]\n",
    "\n",
    "H(S) = -( -0.4422 + -0.5288 ) \\\\[10pt]\n",
    "\n",
    "H(S) = 0.971\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this exact process for all the nodes in our Tree, to have the result you see on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the Information Gain at the root node to see if it was a good decision to split at that node: \n",
    "\n",
    "$\\text{GAIN}(\\mathcal{D}, x_j) = H(\\mathcal{D}) - \\sum_{v \\in \\text{Values}(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Gain}(D, x_i) = H(D) - \\sum_{v \\in V_i} \\frac{|D_v|}{|D|} H(D_v) \\\\[10pt]\n",
    "\n",
    "= 0.971 - \\frac{D_1}{D}H(D_1) - \\frac{D_0}{D}H(D_0) \\\\[10pt]\n",
    "\n",
    "= 0.971 - \\frac{50}{100} \\times 0.971 - \\frac{50}{100} \\times 0.6438 \\\\[10pt]\n",
    "\n",
    "= 0.971 - \\frac{50}{100}(0.971 + 0.6438) \\\\[10pt]\n",
    "\n",
    "= 0.971 - \\frac{1}{2}(0.971 + 0.6438) \\\\[10pt]\n",
    "\n",
    "= 0.971 - 0.5(0.971 + 0.6438) \\\\[10pt]\n",
    "\n",
    "\\text{Gain}(D, x_i) = 0.1636 > 0 \\quad \\text{So it was a good decision}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- D1 is the dataset consisting of values where x1 = 1. We have 50 of these points. <br>\n",
    "\n",
    "- D0 is the dataset consisting of values where x1 = 0 (if we suppose we have only two categories). We have 50 of these points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision tree algorithm will repeat this operation for each feature in the dataset and then choose the feature with the highest Information Gain and choose it as the splitting criterion for the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the formula for the Information Gain i mentionned that H(D) is the entropy, but it can also be **The Gini Entropy**, or the **Misclassification error**:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <div style=\"margin-right: 20px;\">\n",
    "        <img src=\"Gini.png\" alt=\"Gini\" style=\"max-width: 100%;\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"Misclassification.png\" alt=\"Misclassification\" style=\"max-width: 100%;\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; margin-top: 20px;\">\n",
    "    <p>Credit to <a href=\"https://www.linkedin.com/in/sebastianraschka/\" target=\"_blank\">Sebastian Raschka</a></p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy and Gini Impurity are often more used to compute the Information Gain, because Misclassification Error simply looks at the fraction of misclassified instances. It doesn't account for how evenly split the classes are, and can fail to provide the detailed information necessary for effective splitting early in the three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Evaluating the Complexity of Decision Tree Algorithm** <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully at this point, you understand how a Decision Tree algorithm work under the hood. Now the only thing left, is to evaluate the **Algorithm Complexity** let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the complexity of the Decision Tree algorithm, we need to understand the computation happening at each step: \n",
    "\n",
    "- At each node of the decision tree and for every features, we need to find the best splitting criterion. For example, let say we have a feature x1 is it best to take the condition (x1 >1 or x1 = 1) \n",
    "\n",
    "- The best split is the one that maximizes a criterion like **Information Gain**\n",
    "\n",
    "- In order to find the best split, a naive approach will be to test every possible value as a split point, but that would be inefficient. A better way of doing it, will be to first sort the feature values, and then consider only the split point between adjacent values. This will reduces the number of split we need to evaluate to n-1 where n is the number of training examples.\n",
    "\n",
    "- Sorting the feature values for each feature is similar to sorting an array, so the time complexity for that is **O(nlogn)**\n",
    "\n",
    "- Now we repeat this process for each feature in the dataset so the total complexity for a node is **O(d * nlogn)** if we have d features.\n",
    "\n",
    "- But that is just for a node, we will repeat this process for each node. So in order to find the final total complexity we will need to multiply the previous result by the number of node in the tree.\n",
    "\n",
    "- In the worst case scenario a Decision Tree can grow until it has as many leaf nodes as there are training examples(one leaf per sample), that will be n leaf nodes. The total number of nodes is proportional to the number of leaf nodes so proportional to n. In a Binary Decision Tree it is  approximately 2n - 1. In term of complexity both are equivalent to n.\n",
    "\n",
    "- So the final complexity will be **O($n^2$⋅dlogn)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Pros and Cons of Decision Trees** <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
